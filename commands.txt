docker-compose exec namenode bash
hdfs dfs -copyFromLocal data/purchases.txt /user/hadoop/data/purchases/
hdfs dfs -cat /user/hadoop/data/purchases/purchases.txt | more
hdfs dfs -tail /user/hadoop/data/purchases/purchases.txt
hdfs dfs -ls -h /user/hadoop/data/purchases/
hdfs fsck /user/hadoop/data/purchases/purchases.txt -files -blocks
exit

docker-compose exec hive-server bash
/opt/hive/bin/beeline -u jdbc:hive2://localhost:10000
LOAD DATA INPATH '/user/hadoop/data/purchases/purchases.txt' OVERWRITE INTO TABLE purchase;

// namenode
hdfs dfs -ls /user/hive/warehouse/purchase/
hdfs dfs -ls /user/hadoop/data/purchases/

// hive
hive -e 'SET hive.cli.print.header=true; SELECT COUNT(*) AS `total purchase`, `item description` FROM purchase GROUP BY `item description`;' | sed 's/[\t]/,/g'  > ./total_purchase_by_item_description.csv

// namenode
yarn jar /opt/hadoop-2.7.4/share/hadoop/tools/lib/hadoop-streaming-2.7.4.jar \
-files /data/mapper.py,/data/reducer.py \
-mapper /data/mapper.py -reducer /data/reducer.py \
-input /user/hadoop/purchases.txt \
-output /user/hadoop/output-N